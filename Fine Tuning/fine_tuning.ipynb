{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <h2>Fine Tuning Models</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Politics</td>\n",
       "      <td>budget to set scene for election gordon brown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Politics</td>\n",
       "      <td>army chiefs in regiments decision military chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politics</td>\n",
       "      <td>observers to monitor uk election ministers wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Politics</td>\n",
       "      <td>kilroy names election seat target exchat show ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Politics</td>\n",
       "      <td>donor attacks blairbrown feud the reported feu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     Label                                               Text\n",
       "0           0  Politics  budget to set scene for election gordon brown ...\n",
       "1           1  Politics  army chiefs in regiments decision military chi...\n",
       "2           3  Politics  observers to monitor uk election ministers wil...\n",
       "3           4  Politics  kilroy names election seat target exchat show ...\n",
       "4           5  Politics  donor attacks blairbrown feud the reported feu..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"clean_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Sport            505\n",
       "Business         503\n",
       "Politics         403\n",
       "Entertainment    369\n",
       "Technology       347\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace({0:\"Politics\", 1:\"Sport\", 2:\"Technology\", 3:\"Entertainment\", 4:\"Business\"}, inplace=True)\n",
    "data.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data[\"Text\"], data[\"Label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defines a class called **`SentimentDataset`** derived from PyTorch's **`torch.utils.data.Dataset`** class. This class will include the necessary functions to access each item in the dataset and determine its size.\n",
    "- The **`__init__`** function is executed when an instance of the class is created.\n",
    "- **`encodings`**: This parameter represents the tokenized version of the text data (e.g., obtained using BERT tokenization like `train_encodings`).\n",
    "- **`labels`**: This parameter contains the sentiment labels of the text data (e.g., positive, negative, or neutral labels).\n",
    "- The **`__getitem__`** function allows retrieving an item from the dataset using a specific **`idx`** (index).\n",
    "- **`self.encodings.items()`**: Retrieves each item from the tokenized texts and converts each one into a PyTorch tensor with **`torch.tensor(val[idx])`**.\n",
    "- **`item['labels'] = torch.tensor(int(self.labels[idx]))`**: In this line, the label of the text is retrieved and converted to an integer before being added to the **`labels`** key.\n",
    "- The **`__len__`** function returns the size of the dataset. This gives the number of items in the dataset (e.g., equal to the number of labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx])) \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, y_train.tolist())\n",
    "val_dataset = SentimentDataset(val_encodings, y_val.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `BertForSequenceClassification.from_pretrained('bert-base-uncased')`: Bu fonksiyon, BERT modelini 'bert-base-uncased' olarak yükler. Bu model, Wikipedia verisinde eğitilmiş ve case-sensitive (büyük/küçük harf ayrımı yapmayan) bir modeldir. Yani, \"Apple\" ve \"apple\" gibi kelimeleri aynı şekilde ele alır.\n",
    "\n",
    "- `num_labels=5`: Bu parametre, modelin çıktısının 5 farklı etiketten oluşacağını belirtir. Bu, modelin her metin için 5 farklı olası sınıf tahmini yapacağı anlamına gelir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`TrainingArguments`** : Bu sınıf, modelin eğitilmesi için gerekli olan tüm parametreleri içerir.\n",
    "- **`output_dir='./results'`** : Eğitim sonuçlarının (model, günlükler vb.) saklanacağı dizini belirtir.\n",
    "- **`eval_strategy=\"epoch\"`** : Modelin ne zaman değerlendirileceğini belirtir. Burada, değerlendirme her bir **epoch** (tam eğitim geçişi) sonunda yapılacaktır.\n",
    "- **`per_device_train_batch_size=8`** : Her bir cihazda (CPU/GPU) eğitim için kullanılan örnek sayısını belirtir. Bu örnekte her batch (grup) 8 örnekten oluşacaktır.\n",
    "- **`per_device_eval_batch_size=8`** : Değerlendirme sırasında her cihazda kullanılan batch boyutunu belirtir. Burada da batch boyutu 8'dir.\n",
    "- **`num_train_epochs=3`** : Modelin eğitileceği epoch sayısını belirtir. Bu durumda model 3 epoch boyunca eğitilecektir.\n",
    "- **`weight_decay=0.01`** : **Ağırlık çürütme (weight decay)** parametresi, aşırı öğrenmeyi (overfitting) engellemek için kullanılan bir regularization tekniğidir. Modelin her parametresi, her güncellemede %1 (0.01) kadar azalacaktır.\n",
    "\n",
    "---\n",
    "- **`compute_metrics`** : Bu fonksiyon, eğitim ve değerlendirme sırasında modellerin performansını ölçmek için kullanılan metrikleri hesaplar (bu örnekte **doğruluk**).\n",
    "- **`pred.label_ids`** : Gerçek etiketler (hedef değerler) olan doğru etiketler.\n",
    "- **`pred.predictions`** : Modelin tahminleri.\n",
    "- **`np.argmax(pred.predictions, axis=1)`** : Modelin yaptığı tahminlerde, her örnek için en yüksek olasılığa sahip sınıfın indeksini alır.\n",
    "- **`accuracy_score(labels, preds)`** : Gerçek etiketler ile modelin tahminleri arasındaki doğruluğu hesaplar.\n",
    "---\n",
    "- **`Trainer`** : **Trainer** sınıfı, Hugging Face tarafından sağlanan ve modelin eğitilmesini yöneten bir sınıftır.\n",
    "- **`model=model`** : Eğitilecek model (bu durumda yüklü olan BERT modeli).\n",
    "- **`args=training_args`** : Önceden yapılandırılmış eğitim parametreleri (batch boyutu, epoch sayısı, vb.).\n",
    "- **`train_dataset=train_dataset`** : Modelin eğitim için kullanacağı veri seti.\n",
    "- **`eval_dataset=val_dataset`** : Modelin her epoch sonunda performansını değerlendireceği doğrulama veri seti.\n",
    "- **`compute_metrics=compute_metrics`** : Eğitim ve değerlendirme sırasında kullanılacak metrik hesaplama fonksiyonu (burada doğruluk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458a2233a9d847bf9c4f7712751e960e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3875180b8d014886b395466709334fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07627822458744049, 'eval_accuracy': 0.9859154929577465, 'eval_runtime': 53.5089, 'eval_samples_per_second': 7.961, 'eval_steps_per_second': 1.009, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e487e53fb8674a92a9114bd91764e514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12288319319486618, 'eval_accuracy': 0.9788732394366197, 'eval_runtime': 51.1887, 'eval_samples_per_second': 8.322, 'eval_steps_per_second': 1.055, 'epoch': 2.0}\n",
      "{'loss': 0.1486, 'grad_norm': 0.014019527472555637, 'learning_rate': 1.0876369327073553e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b4998becc74460bb35fbc75c8593b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09346222877502441, 'eval_accuracy': 0.9812206572769953, 'eval_runtime': 52.8672, 'eval_samples_per_second': 8.058, 'eval_steps_per_second': 1.021, 'epoch': 3.0}\n",
      "{'train_runtime': 2700.3861, 'train_samples_per_second': 1.89, 'train_steps_per_second': 0.237, 'train_loss': 0.12089223406497675, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743e54af93c544b78d6605850fe1cdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9812206572769953\n",
      "Loss: 0.09346222877502441\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']}\")\n",
    "print(f\"Loss: {eval_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: Politics\n"
     ]
    }
   ],
   "source": [
    "test_text = [\"The government has announced a new policy aimed at reducing carbon emissions by 50% by 2030.\"]\n",
    "test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "outputs = model(**test_encodings)\n",
    "preds = torch.argmax(outputs.logits, dim=1)\n",
    "predicted_label = label_encoder.inverse_transform(preds.numpy())\n",
    "print(f\"Predicted label: {predicted_label[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model\\\\tokenizer_config.json',\n",
       " './saved_model\\\\special_tokens_map.json',\n",
       " './saved_model\\\\vocab.txt',\n",
       " './saved_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "model = BertForSequenceClassification.from_pretrained('./saved_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `RoBERTa`, 2019 yılında Facebook tarafından geliştirilen ve BERT’in geliştirilmiş bir versiyonudur. BERT ile aynı mimariye sahiptir ancak bazı eğitim süreci optimizasyonları yapılmıştır: RoBERTa, daha fazla veri üzerinde, daha uzun sürelerde ve BERT'teki bazı sınırlamaların (örneğin, sonraki cümle tahmini gibi) kaldırılmasıyla eğitilmiştir. Bu iyileştirmeler sayesinde RoBERTa, karmaşık ilişkileri daha iyi öğrenir ve birçok NLP görevinde, özellikle metin sınıflandırma gibi alanlarda daha yüksek performans gösterir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings_roberta = roberta_tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "val_encodings_roberta = roberta_tokenizer(list(X_val), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_roberta = SentimentDataset(train_encodings_roberta, y_train.tolist())\n",
    "val_dataset_roberta = SentimentDataset(val_encodings_roberta, y_val.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir='./results_roberta',\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_roberta = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args_roberta,\n",
    "    train_dataset=train_dataset_roberta,\n",
    "    eval_dataset=val_dataset_roberta,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a62ab3da135451e834d0983458b298b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e29a5cabb145e9b68301d39ebf7882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12714247405529022, 'eval_accuracy': 0.9765258215962441, 'eval_runtime': 46.846, 'eval_samples_per_second': 9.094, 'eval_steps_per_second': 1.153, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1c40fb447b4939b9d30ee184887d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1554717719554901, 'eval_accuracy': 0.9788732394366197, 'eval_runtime': 45.2472, 'eval_samples_per_second': 9.415, 'eval_steps_per_second': 1.193, 'epoch': 2.0}\n",
      "{'loss': 0.2, 'grad_norm': 0.013453349471092224, 'learning_rate': 1.0876369327073553e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd1c20b7d29416eb077e0fb677a1957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1385667771100998, 'eval_accuracy': 0.9765258215962441, 'eval_runtime': 43.9853, 'eval_samples_per_second': 9.685, 'eval_steps_per_second': 1.228, 'epoch': 3.0}\n",
      "{'train_runtime': 2472.2344, 'train_samples_per_second': 2.064, 'train_steps_per_second': 0.258, 'train_loss': 0.16818640265666263, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ff26224b2f484d8abe4369fa7ce7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Accuracy: 0.9765258215962441\n",
      "RoBERTa Loss: 0.1385667771100998\n"
     ]
    }
   ],
   "source": [
    "trainer_roberta.train()\n",
    "\n",
    "eval_results_roberta = trainer_roberta.evaluate()\n",
    "print(f\"RoBERTa Accuracy: {eval_results_roberta['eval_accuracy']}\")\n",
    "print(f\"RoBERTa Loss: {eval_results_roberta['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model_roberta\\\\tokenizer_config.json',\n",
       " './saved_model_roberta\\\\special_tokens_map.json',\n",
       " './saved_model_roberta\\\\vocab.json',\n",
       " './saved_model_roberta\\\\merges.txt',\n",
       " './saved_model_roberta\\\\added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model.save_pretrained('./saved_model_roberta')\n",
    "roberta_tokenizer.save_pretrained('./saved_model_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_choice(text, model_choice):\n",
    "    if model_choice == \"BERT\":\n",
    "        # Use BERT for prediction\n",
    "        inputs = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "    elif model_choice == \"RoBERTa\":\n",
    "        # Use RoBERTa for prediction\n",
    "        inputs = roberta_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "        outputs = roberta_model(**inputs)\n",
    "\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    predicted_label = label_encoder.inverse_transform(preds.numpy())\n",
    "    return predicted_label[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=predict_with_choice,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Text to Classify\"),  \n",
    "        gr.Dropdown(choices=[\"BERT\", \"RoBERTa\"], label=\"Choose Model\"),  \n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Text Classification with BERT and RoBERTa\",\n",
    "    description=\"Enter a text and select a model (BERT or RoBERTa) to predict its category.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
